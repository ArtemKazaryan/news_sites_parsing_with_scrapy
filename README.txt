1. Откройте папку с проектом в PyCharm или VSCode.
2. Откройте терминал.
3. Для перехода в папку запуска Введите команду cd newsparse/newsparse/spiders и нажмите ENTER.
4. Для запуска процесса парсинга введите команду scrapy crawl news_spider и нажмите ENTER.
5. В терминале появится меню, в котором Вам будет предложен выбор. Укажите выбранный пункт и нажмите ENTER.
6. При выборе одного из сайтов запустится кроулинг и потребуется выждать некоторое время, которое зависит от выставленных параметров сайта.
7. По окончании данные по новостям запишутся в таблицу items базы данных parsenewse.db, которые можно просмотреть в SQLiteStudio, например.
8. Если потребуется дополнительная запись данных в файл json, то такая команда как scrapy crawl news_spider -O news.jsonв запишет данные в файл news.json.
9. В атрибуте resource_url таблицы resource имеется параметр перехода по страницам (например, pagination 1133 1130 -1, где 1133 - начальная страница, 
   1130 - конечная, -1 -шаг. Возможен обратный порядок пагинации pagination 1 5 1) или дозагрузки по скруллу мыши (например, scrolling 1 2, где 1 - число полных дозагрузок,
   а 2 - время в секундах ожидания полной подгрузки с помощью движка Selenium).
10. Предусмотренно только ручное удаление данных из БД (Как впрочем и заполнение данных).